---
title: "IMDB_Hypertuning"
author: "Harika"
date: "2/4/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Loading IMDB dataset from keras.
```{r}

library(keras)

imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb

vectorize_sequences <- function(sequences, dimension = 10000) {
  # Create an all-zero matrix of shape (len(sequences), dimension)
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    # Sets specific indices of results[i] to 1s
    results[i, sequences[[i]]] <- 1
  results
}

# Our vectorized training data
train <- vectorize_sequences(train_data)
# Our vectorized test data
test <- vectorize_sequences(test_data)

# Our vectorized labels
train_label <- as.numeric(train_labels)
test_label <- as.numeric(test_labels)


```

#Divide training set into train and validation set for validating purposes.
```{r}
set.seed(123)
val_indices <- 1:10000

x_val <- train[val_indices,]
x_train <-train[-val_indices,]

y_val <- train_label[val_indices]
y_train <- train_label[-val_indices]
```

#Model building (neural network) with initial model setting/parameters as layers=3,hidden units=16,activation functions as "relu",Loss function "binary cross entropy" and optimizer as rmsprop.


```{r}
initial_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

initial_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- initial_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
plot(history)

val <-initial_model %>% evaluate(x_val,y_val)
val

# Retrain the network

initial_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

initial_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

initial_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_initial <- initial_model %>% evaluate(test,test_label)
results_initial
```

#Different hidden units like 4,64,128

#Now lets start with hidden units as 4 and all other parameters are same as initial model

```{r}
units4_model <- keras_model_sequential() %>% 
  layer_dense(units = 4, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

units4_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
history_4units <- units4_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_4units)
val <-units4_model %>% evaluate(x_val,y_val)
val
#Retrain the model

units4_model <- keras_model_sequential() %>% 
  layer_dense(units = 4, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

units4_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
units4_model%>% fit(train, train_label, epochs = 2, batch_size = 512)
results_units4 <- units4_model %>% evaluate(test,test_label)
results_units4

```

#Hidden units as 64 and remaining parameters same as initial model
```{r}
units64_model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

units64_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
history_64units <- units64_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_64units)
val <-units64_model %>% evaluate(x_val,y_val)
val

#Retrain the network

units64_model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

units64_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
units64_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_units64 <- units64_model %>% evaluate(test,test_label)
results_units64
```

#Increase more hidden units like to 128.
```{r}
units128_model <- keras_model_sequential() %>% 
  layer_dense(units = 128, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

units128_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
history_128units <- units128_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_128units)

val <-units64_model %>% evaluate(x_val,y_val)
val

#Retrain the network

units128_model <- keras_model_sequential() %>% 
  layer_dense(units = 128, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

units128_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

units128_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_units128 <- units128_model %>% evaluate(test,test_label)
results_units128
```

#Plotting different hidden units losses
```{r}
library(ggplot2)
library(tidyr)
plot_val_losses <- function(losses) {
  loss_names <- names(losses)
  losses <- as.data.frame(losses)
  losses$epoch <- seq_len(nrow(losses))
  losses %>% 
    gather(model, loss, loss_names[[1]], loss_names[[2]],loss_names[[3]],loss_names[[4]]) %>% 
    ggplot(aes(x = epoch, y = loss)) +
    geom_point(aes(colour = model))
}
plot_val_losses(losses = list(
  initial_model = history$metrics$val_loss,
  units4_model = history_4units$metrics$val_loss,
  units64_model = history_64units$metrics$val_loss,
  units128_model = history_128units$metrics$val_loss)) 
```
# smaller hidden units of 4 model starts slightly overfitting much later than initial model and when hidden units are increased to 64 or 128 the models overfits early like with 128 units loss is increased in third epoch itself.
#mse loss function

#Now changing loss function from binary cross entropy to "mse" and all other parameters as same.

```{r}
mse_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

mse_model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history_mse <- mse_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_mse)
val <-mse_model %>% evaluate(x_val,y_val)
val

#Retrain the network
mse_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

mse_model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)
mse_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_mse <- mse_model %>% evaluate(test,test_label)
results_mse
```
#Using “mse” as loss function for IMDB dataset, loss value is low when compared to binary cross entropy. MSE is also good measure for data of form normal distribution. Let’s examine with an example
If actual and predicted labels are same, then both loss functions give error as “0”.
Actual label: 1
Predicted label: 1 
MSE: (1 - 1) ² = 0
Cross-entropy: -(1 * log (1) + 0 * log (0)) = 0

Consider different scenario like
Actual label: 1
Predicted label: 0
MSE: (1 - 1) ² = 1
Cross-entropy: -(1 * log (0) + 0 * log (1)) = tends to infinity
Here “mse” loss function error is less compared to cross entropy loss function. Cross entropy strongly penalizes the misclassifications.

#Plots for loss and accuracy for initial model and mse model.
```{r}
library(ggplot2)
library(tidyr)

plot_val_losses <- function(losses) {
  loss_names <- names(losses)
  losses <- as.data.frame(losses)
  losses$epoch <- seq_len(nrow(losses))
  losses %>% 
    gather(model, loss, loss_names[[1]], loss_names[[2]]) %>% 
    ggplot(aes(x = epoch, y = loss))+
    geom_point(aes(colour = model))
  
}
plot_val_losses(losses = list(
  initial_model = history$metrics$val_loss,
  mse_model = history_mse$metrics$val_loss
 ))

plot_val_accuracy <- function(accuracy) {
  accuracy_names <- names(accuracy)
  accuracy <- as.data.frame(accuracy)
  accuracy$epoch <- seq_len(nrow(accuracy))
  accuracy %>% 
    gather(model, accuracy, accuracy_names[[1]], accuracy_names[[2]]) %>% 
    ggplot(aes(x = epoch, y = accuracy))+
    geom_point(aes(colour = model))
}
plot_val_accuracy(accuracy = list(
  initial_model = history$metrics$val_accuracy,
  mse_model = history_mse$metrics$val_accuracy
 ))
```

#In above graphs we can clearly notice that loss is less and accuracy is also same for mse as loss function model.

#tanh activation function

#Using tanh instead of ReLu activation function
```{r}
tanh_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "tanh", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "tanh") %>% 
  layer_dense(units = 1, activation = "sigmoid")

tanh_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_tanh <- tanh_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_tanh)
val <-tanh_model %>% evaluate(x_val,y_val)
val
#Retrain the network

tanh_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "tanh", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "tanh") %>% 
  layer_dense(units = 1, activation = "sigmoid")

tanh_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
tanh_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_tanh <- tanh_model %>% evaluate(test,test_label)
results_tanh
```

#PLotting loss and accuracy
```{r}
plot_val_losses(losses = list(
  initial_model = history$metrics$val_loss,
  tanh_model = history_tanh$metrics$val_loss
 ))

plot_val_accuracy(accuracy  = list(
  initial_model = history$metrics$val_accuracy,
  tanh_model = history_tanh$metrics$val_accuracy
 ))
```
#The accuracy of tanh activation function for the model is low and also degrades performance due to vanishing gradient problem.
#L-1 regularization
#L-1 norm regularization is cost added is proportional to the absolute values of weight coefficients.

```{r}
L1_model <- keras_model_sequential() %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l1(0.001), activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l1(0.001), activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

L1_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_L1 <- L1_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_L1)
val <-initial_model %>% evaluate(x_val,y_val)
val

#
L1_model <- keras_model_sequential() %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l1(0.001), activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l1(0.001), activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

L1_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
L1_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_L1 <- L1_model %>% evaluate(test,test_label)
results_L1
```
#Plotting loss and accuracy for validation dataset
```{r}

plot_val_losses(losses = list(
  initial_model = history$metrics$val_loss,
  L1_model = history_L1$metrics$val_loss
))

plot_val_accuracy(accuracy = list(
  initial_model = history$metrics$val_accuracy,
  L1_model = history_L1$metrics$val_accuracy
))
```

#L-1 model is resistant to overfitting than initial model.Accuracy is almost the same with small differnces in each epoch.

#L-2 regularization
#L-2 norm regularization-cost added is proportional to square of the value of weight coefficients.

```{r}
L2_model <- keras_model_sequential() %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l2(0.001), activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l2(0.001), activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

L2_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_L2 <- L2_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_L2)
val <-L2_model %>% evaluate(x_val,y_val)
val

L2_model <- keras_model_sequential() %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l2(0.001), activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16,kernel_regularizer = regularizer_l2(0.001), activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

L2_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
L2_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_L2 <- L2_model %>% evaluate(test,test_label)
results_L2
```

#plot for loss and accuracy
```{r}

plot_val_losses(losses = list(
  initial_model = history$metrics$val_loss,
  L2_model = history_L2$metrics$val_loss
))

plot_val_accuracy(accuracy = list(
  initial_model = history$metrics$val_accuracy,
  L2_model = history_L2$metrics$val_accuracy
))
```
#The graph for L-2 model shows loss is significantly low and also less overfitting compared to initial model.Coming to the accuracy the L-2 model showing better accuracy.

#Differnt number of Hidden Layers 
# 3 hidden layers
```{r}
layers3_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

layers3_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_layers3 <- layers3_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_layers3)
val <-layers3_model %>% evaluate(x_val,y_val)
val

#re train
layers3_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

layers3_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
layers3_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_layers3 <- layers3_model %>% evaluate(test,test_label)
results_layers3
```

#Plot
```{r}
plot_val_losses(losses = list(
  initial_model = history$metrics$val_loss,
  layers3_model = history_layers3$metrics$val_loss
))

plot_val_accuracy(accuracy = list(
  initial_model = history$metrics$val_accuracy,
  layers3_model = history_layers3$metrics$val_accuracy
))
```

#one hidden layer
```{r}
layers1_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 1, activation = "sigmoid")

layers1_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_layers1 <- layers1_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

plot(history_layers1)
val <-layers1_model %>% evaluate(x_val,y_val)
val
#re-train
layers1_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 1, activation = "sigmoid")

layers1_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

layers1_model%>% fit(train, train_label, epochs = 4, batch_size = 512)
results_layers1 <- layers1_model %>% evaluate(test,test_label)
results_layers1
```

# Plot for loss and accuarcy
```{r}
plot_val_losses(losses = list(
  initial_model = history$metrics$val_loss,
  layers1_model = history_layers1$metrics$val_loss
))

plot_val_accuracy(accuracy = list(
  initial_model = history$metrics$val_accuracy,
  layers1_model = history_layers1$metrics$val_accuracy
))
```
